import argparse
import datetime
import json
import logging
import os
import re
import sys
import textwrap

from hashlib import sha256

import requests

###############################################################################
# <Backup strategy>
# Backup will be placed in the application directory in Dropbox, structured as:
# - site-xxx
#   |-- backup-20200503-1200
#   |   |-- readme.txt
#   |   |-- backup.tar.gz
#   |   |-- backup.tar.gz.1
#   |   `-- backup.tar.gz.2
#   `-- backup-20200504-1200
#   |   |-- readme.txt
#       |-- backup.tar.gz
#       |-- backup.tar.gz.1
#       `-- backup.tar.gz.2
# where `site-xxx` is args.directory.
#
# Backup data is read through stdin to save disk and memory consumption.
# `tar zcf - ... ... | python -u upload_dropbox.py site-xxx
#
# After creating a backup directory, it checks all the existing backups in
# args.directory, sorts by directory name, and deletes old directories.
#
# There could be multiple ".tar.gz.[0-9]+" files in directory as shown above.
# This is because Dropbox requires each file to be smaller than 10GB.
# When restoring the backup, simply concatenate .tar.gz, tar.gz.1, ... into
# one single giant tar.gz.
#
# This giant tar.gz file can be passed to restore_wp_backup.sh
#
# <Dropbox file-upload model>
# Dropbox requires a large file to be splitted and separately sent in series.
# 1. First open upload session, 2. Upload each blocks, 3. Finalize the session
# A small file can be uploaded by a single API call,
# which is implemeted in upload_file_single
###############################################################################

# Dropbox filename has to follow this regex pattern
# https://dropbox.com/developers/documentation/http/documentation#files-upload
dest_pattern = r'(/(.|[\r\n])*)|(ns:[0-9]+(/.*)?)|(id:.*)'

# filesize_limit = 10 * (1024 ** 3)       # 10GB/file
filesize_limit = 10 * (1024 ** 3)

# Used to generate
dirname_format = 'backup-%Y%m%d-%H%M'

# Used to search previous backup files in Dropbox
# that are generated by dirname_format
dirname_pattern = r'^backup-[0-9]{8}-[0-9]{4}$'

logger = logging.getLogger('upload_dropbox')


class SlackHandler(logging.Handler):
    def emit(self, record):
        entry = self.format(record)
        payload = {
            'token': os.environ['SLACK_TOKEN'],
            'channel': os.environ['SLACK_CHANNEL'],
            'text': entry
        }
        ep = 'https://slack.com/api/chat.postMessage'
        requests.post(ep, data=payload)


def config_logger(logger):
    logger.setLevel(logging.INFO)
    fmt = "%(asctime)s `%(name)s` [%(levelname)s] - %(message)s"
    slack_handler = SlackHandler()
    slack_handler.setFormatter(logging.Formatter(fmt))
    logger.addHandler(slack_handler)

    fmt = "%(asctime)s %(name)s [%(levelname)s] - %(message)s"
    stream_handler = logging.StreamHandler(sys.stderr)
    stream_handler.setFormatter(logging.Formatter(fmt))
    logger.addHandler(stream_handler)


def post(ep, token, arg, data=None):
    headers = {
        "Authorization": "Bearer {}".format(token),
        "Dropbox-API-Arg": json.dumps(arg),
        "Content-Type": "application/octet-stream"
    }
    return requests.post('https://content.dropboxapi.com/2/files{}'.format(ep),
                         headers=headers, data=data)


def open_session(token):
    arg = {"close": False}
    res = post('/upload_session/start', token, arg)
    if res.status_code != 200:
        logger.error("Failed to open an upload-session.")
        logger.error('Request Dropbox-API-Arg: {}'.format(arg))
        logger.error('Response: {}'.format(res.text))
        exit(-1)
    res = json.loads(res.text)
    return res['session_id']


def upload_file_single(token, data, filename):
    arg = {
        "path": filename,
        "mode": "add",
        "autorename": True,
        "mute": False,
        "strict_conflict": False
    }
    res = post('/upload', token, arg, data)
    if res.status_code != 200:
        logger.error("Upload failed.")
        logger.error('Request Dropbox-API-Arg: {}'.format(arg))
        logger.error('Response: {}'.format(res.text))
        exit(-1)


def upload_part(token, session_id, data, offset):
    arg = {
        "cursor": {"session_id": session_id, "offset": offset},
        "close": False
    }
    res = post('/upload_session/append_v2', token, arg, data)
    if res.status_code != 200:
        logger.error("Upload block failed.")
        logger.error('Request Dropbox-API-Arg: {}'.format(arg))
        logger.error('Response: {}'.format(res.text))
        exit(-1)
    return offset + len(data)


def finish_session(token, session_id, offset, dest_filename, hasher):
    arg = {
        "cursor": {"session_id": session_id, "offset": offset},
        "commit": {
            "path": dest_filename,
            "mode": "add",
            "autorename": False,
            "mute": False,
            "strict_conflict": False
        }
    }
    res = post('/upload_session/finish', token, arg)
    if res.status_code != 200:
        logger.error("Upload completion process failed.")
        logger.error('Request Dropbox-API-Arg: {}'.format(arg))
        logger.error('Response: {}'.format(res.text))
        exit(-1)

    res = json.loads(res.text)
    if not res['content_hash'] == hasher.hash or res['size'] != offset:
        logger.error("Uploaded file corrupted, although the API completed"
                     " (size or hash mismatch)")
        exit(-1)
    report = "Upload completed. Path: {}, Size: {}bytes" \
             .format(res['path_display'], res['size'])
    logger.info(report)


def recent_backups(token, directory):
    headers = {
        "Authorization": "Bearer {}".format(token),
        "Content-Type": "application/json",
    }
    payload = {
        "path": directory,
        "recursive": False,
        "include_media_info": False,
        "include_deleted": False,
        "include_has_explicit_shared_members": False,
        "include_mounted_folders": True,
        "include_non_downloadable_files": True
    }
    res = requests.post('https://api.dropboxapi.com/2/files/list_folder',
                        headers=headers, data=json.dumps(payload))
    if res.status_code != 200:
        logger.error('List backup directory failed')
        logger.error(res.text)
        exit(1)
    res = json.loads(res.text)

    results = [ent for ent in res['entries']
               if ent['.tag'] == 'folder' and
               re.match(dirname_pattern, ent['name'])]
    return sorted(results, key=lambda ent: ent['name'])[::-1]


def remove_dir(token, directory):
    headers = {
        "Authorization": "Bearer {}".format(token),
        "Content-Type": "application/json",
    }
    payload = {"path": directory}
    res = requests.post('https://api.dropboxapi.com/2/files/delete_v2',
                        headers=headers, data=json.dumps(payload))
    if res.status_code != 200:
        logger.error("Deletion of {} failed".format(directory))
        logger.error(res.text)
    else:
        logger.info("Successfully delete {}".format(directory))


class Hasher:
    """File hash calculator

    Dropbox file upload API returns hash of the uploaded file,
    which is calculated by a bit complicated logic described in
    https://www.dropbox.com/developers/reference/content-hash
    This class is the efficient implementation.
    """
    def __init__(self):
        self.locked = False
        self.block_hashes = b''
        self.current_block = b''
        self.unit = 4 * 1024 * 1024

    def __call__(self, data):
        assert not self.locked
        assert isinstance(data, bytes)
        self.current_block += data
        while self.unit <= len(self.current_block):
            block = self.current_block[:self.unit]
            self.current_block = self.current_block[self.unit:]
            self.block_hashes += sha256(block).digest()

    @property
    def hash(self):
        if len(self.current_block):
            self.block_hashes += sha256(self.current_block).digest()
            self.current_block = b''
            self.locked = True
        return sha256(self.block_hashes).hexdigest()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('directory', type=str,
                        help='Destination file. Needs to be "/path/to/"')
    parser.add_argument('--n-retain', type=int, default=5,
                        help="Log rotation")
    parser.add_argument('--block-size-mb', type=int, default=128)
    args = parser.parse_args()

    block_size = args.block_size_mb * (1024 ** 2)
    config_logger(logger)

    if args.directory == '/':
        args.directory = ''

    dirname = datetime.datetime.now().strftime(dirname_format)
    dest = os.path.join(args.directory, dirname, 'backup.tar.gz')
    match = re.match(dest_pattern, dest)
    if not match:
        logger.error('Destination "{}" does not meet the format requirement {}'
                     .format(dest, dest_pattern))
        exit(-1)

    if 'DROPBOX_TOKEN' not in os.environ:
        logger.error('"DROPBOX_TOKEN" environment variable not set')
        exit(-1)
    token = os.environ['DROPBOX_TOKEN']

    # Start upload session
    session_id = open_session(token)

    # Batch upload process
    logger.info("Starting backup to {}".format(dest))
    hasher = Hasher()
    offset, i = 0, 0
    batch = 0
    _dest = dest
    while True:
        s = sys.stdin.buffer.read(block_size)
        if not s:
            # Regular completion proess!
            finish_session(token, session_id, offset, _dest, hasher)
            break
        logger.info("Upload batch {} - {} bytes".format(i, len(s)))
        offset = upload_part(token, session_id, s, offset)
        hasher(s)
        i += 1

        # If it'll exceed the file size limit of 10GB in the next block
        if filesize_limit < offset + block_size:
            logger.info("About to exceed 10GB!! Complete this batch")
            finish_session(token, session_id, offset, _dest, hasher)

            # Open new session
            batch += 1
            _dest = '{}.{}'.format(dest, batch)
            logger.info("Next batch -> {}".format(_dest))
            offset = 0
            hasher = Hasher()
            session_id = open_session(token)

    # Leave a note
    readme = """
    Wordpress backup

    Restore proces
    (1) Downlad all the "tar.gz.[0-9]+" files to local.
    (2) Concatenate them into a single giant tar.gz.
    (3) Send it to the blog server.
    (4) Pass it to restore_wp_backup.sh
    Example:
    sudo bash backup_scripts/restore_wp_backup.sh site-xxxx /site-xxxx/backup-yyyymmdd-hhmm
    """  # NOQA
    readme = textwrap.dedent(readme)
    upload_file_single(token, readme.encode('utf-8'),
                       os.path.join(args.directory, dirname, 'readme.txt'))

    # Delete old logs!!
    entries = recent_backups(token, args.directory)
    logger.info('Found {} previous backups: {}'
                .format(len(entries),
                        ", ".join(ent['path_display'] for ent in entries)))
    for ent in entries[args.n_retain:]:
        remove_dir(token, ent['path_display'])
    logger.info("Backup process completed")


if __name__ == '__main__':
    main()
